{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "tensorflow version 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "print('tensorflow version', tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length:  254\n",
      "class:  1\n",
      "data size:  26709\n",
      "former versace store clerk sues over secret 'black code' for minority shoppers\n",
      "->\n",
      "[ 308    1  679 3337 2298   48  382 2576    1    6 2577    1    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "token:  (26709, 46)\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "  headline = []\n",
    "  label = []\n",
    "  with open('sarcasm/sarcasm.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "    for entry in raw:\n",
    "      headline.append(entry['headline'])\n",
    "      label.append(entry['is_sarcastic'])\n",
    "  max_length = max([len(h) for h in headline])\n",
    "  print('max_length: ', max_length)\n",
    "  assert len(headline) == len(label)\n",
    "  max_classes = max([l for l in label])\n",
    "  print('class: ', max_classes)\n",
    "  return headline, label\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 6000\n",
    "MAX_LENGTH = 46\n",
    "\n",
    "data, label = load_data()\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNKNOWN_TOKEN>',\n",
    "                                                  num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(data)\n",
    "print('data size: ', len(data))\n",
    "# print(tokenizer.word_index)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "tokens = keras.preprocessing.sequence.pad_sequences(sequences,\n",
    "                                                    padding='post',\n",
    "                                                    truncating='post',\n",
    "                                                    maxlen=MAX_LENGTH)\n",
    "assert len(tokens) == len(label)\n",
    "print(data[0])\n",
    "print('->')\n",
    "print(tokens[0])\n",
    "print('token: ', tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 308    1  679 3337 2298   48  382 2576    1    6 2577    1    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0], shape=(46,), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "training_size = int(len(data) * 0.8)\n",
    "validation_size = len(data) - training_size\n",
    "ds = tf.data.Dataset.from_tensor_slices((tokens, label))\n",
    "training_ds = ds.take(training_size).batch(BATCH_SIZE)\n",
    "validation_ds = ds.skip(training_size).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "for example in training_ds.take(1):\n",
    "  example_data, example_label = example\n",
    "  print(example_data[0])\n",
    "  print(example_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 46)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 46, 16)       96000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 46, 16)]     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 46, 16)       8208        tf_op_layer_AddV2[0][0]          \n",
      "                                                                 tf_op_layer_AddV2[0][0]          \n",
      "                                                                 tf_op_layer_AddV2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 46, 16)       0           multi_head_attention[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 46, 16)       0           tf_op_layer_AddV2[0][0]          \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 46, 16)       32          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 46, 16)       272         layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 46, 16)       32          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 46, 16)       0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 16)           0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           340         global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            42          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 104,926\n",
      "Trainable params: 104,926\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 16\n",
    "\n",
    "\n",
    "def embedding(x):\n",
    "  position = tf.range(start=0, limit=MAX_LENGTH, delta=1)\n",
    "  em = layers.Embedding(VOCAB_SIZE, EMBEDDING_SIZE, mask_zero=True)(x)\n",
    "  pos_em = layers.Embedding(MAX_LENGTH, EMBEDDING_SIZE)(position)\n",
    "  return em + pos_em\n",
    "\n",
    "\n",
    "def transformer(x, dropout=0.2):\n",
    "  m = tfa.layers.MultiHeadAttention(EMBEDDING_SIZE, 8)([x, x, x])\n",
    "  d = layers.Dropout(dropout)(m)\n",
    "  a = layers.Add()([x, d])\n",
    "  x = layers.LayerNormalization(epsilon=1e-6)(a)\n",
    "  x = layers.Dense(EMBEDDING_SIZE, activation='relu')(x)\n",
    "  x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "  x = layers.Dropout(dropout)(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "def build_model():\n",
    "  inputs = keras.Input(shape=[MAX_LENGTH], dtype=tf.int32)\n",
    "  em = embedding(inputs)\n",
    "  x = transformer(em)\n",
    "  x = layers.GlobalAveragePooling1D()(x)\n",
    "  x = layers.Dense(20, activation=\"relu\")(x)\n",
    "  outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "  model = keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "  model.compile(optimizer=keras.optimizers.Adam(1e-4),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "\n",
    "keras.backend.clear_session()\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  2/167 [..............................] - ETA: 6s - loss: 0.9136 - accuracy: 0.3984WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0055s vs `on_train_batch_end` time: 0.0695s). Check your callbacks.\n",
      "165/167 [============================>.] - ETA: 0s - loss: 0.6986 - accuracy: 0.5324\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.56870, saving model to sarcasm-model/sarcasm.01-0.67.h5\n",
      "167/167 [==============================] - 1s 8ms/step - loss: 0.6983 - accuracy: 0.5327 - val_loss: 0.6702 - val_accuracy: 0.5687\n",
      "Epoch 2/10\n",
      "162/167 [============================>.] - ETA: 0s - loss: 0.6607 - accuracy: 0.6005\n",
      "Epoch 00002: val_accuracy improved from 0.56870 to 0.66455, saving model to sarcasm-model/sarcasm.02-0.64.h5\n",
      "167/167 [==============================] - 1s 8ms/step - loss: 0.6602 - accuracy: 0.6020 - val_loss: 0.6437 - val_accuracy: 0.6645\n",
      "Epoch 3/10\n",
      "167/167 [==============================] - ETA: 0s - loss: 0.6171 - accuracy: 0.7113\n",
      "Epoch 00003: val_accuracy improved from 0.66455 to 0.78061, saving model to sarcasm-model/sarcasm.03-0.58.h5\n",
      "167/167 [==============================] - 1s 8ms/step - loss: 0.6171 - accuracy: 0.7113 - val_loss: 0.5832 - val_accuracy: 0.7806\n",
      "Epoch 4/10\n",
      "166/167 [============================>.] - ETA: 0s - loss: 0.5104 - accuracy: 0.8009\n",
      "Epoch 00004: val_accuracy improved from 0.78061 to 0.82385, saving model to sarcasm-model/sarcasm.04-0.44.h5\n",
      "167/167 [==============================] - 2s 10ms/step - loss: 0.5101 - accuracy: 0.8010 - val_loss: 0.4433 - val_accuracy: 0.8238\n",
      "Epoch 5/10\n",
      "165/167 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.8512\n",
      "Epoch 00005: val_accuracy improved from 0.82385 to 0.84238, saving model to sarcasm-model/sarcasm.05-0.37.h5\n",
      "167/167 [==============================] - 1s 7ms/step - loss: 0.3782 - accuracy: 0.8515 - val_loss: 0.3743 - val_accuracy: 0.8424\n",
      "Epoch 6/10\n",
      "167/167 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.8739\n",
      "Epoch 00006: val_accuracy improved from 0.84238 to 0.84538, saving model to sarcasm-model/sarcasm.06-0.36.h5\n",
      "167/167 [==============================] - 1s 8ms/step - loss: 0.3118 - accuracy: 0.8739 - val_loss: 0.3568 - val_accuracy: 0.8454\n",
      "Epoch 7/10\n",
      "161/167 [===========================>..] - ETA: 0s - loss: 0.2759 - accuracy: 0.8879\n",
      "Epoch 00007: val_accuracy improved from 0.84538 to 0.84893, saving model to sarcasm-model/sarcasm.07-0.36.h5\n",
      "167/167 [==============================] - 1s 8ms/step - loss: 0.2765 - accuracy: 0.8879 - val_loss: 0.3558 - val_accuracy: 0.8489\n",
      "Epoch 8/10\n",
      "165/167 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.9000\n",
      "Epoch 00008: val_accuracy did not improve from 0.84893\n",
      "167/167 [==============================] - 1s 7ms/step - loss: 0.2509 - accuracy: 0.9001 - val_loss: 0.3629 - val_accuracy: 0.8480\n",
      "Epoch 9/10\n",
      "162/167 [============================>.] - ETA: 0s - loss: 0.2317 - accuracy: 0.9077\n",
      "Epoch 00009: val_accuracy did not improve from 0.84893\n",
      "167/167 [==============================] - 1s 8ms/step - loss: 0.2319 - accuracy: 0.9074 - val_loss: 0.3756 - val_accuracy: 0.8459\n",
      "Epoch 10/10\n",
      "162/167 [============================>.] - ETA: 0s - loss: 0.2163 - accuracy: 0.9161\n",
      "Epoch 00010: val_accuracy did not improve from 0.84893\n",
      "167/167 [==============================] - 1s 7ms/step - loss: 0.2167 - accuracy: 0.9159 - val_loss: 0.3829 - val_accuracy: 0.8459\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = callbacks.TensorBoard()\n",
    "checkpoint_callback = callbacks.ModelCheckpoint('sarcasm-model/sarcasm.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "                                                monitor='val_accuracy',\n",
    "                                                verbose=1,\n",
    "                                                save_best_only=True)\n",
    "\n",
    "\n",
    "history = model.fit(training_ds, epochs=10, validation_data=validation_ds,\n",
    "                    callbacks=[tensorboard_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiddos/.local/lib/python3.6/site-packages/tensorflowjs/converters/keras_h5_conversion.py:123: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  return h5py.File(h5file)\n"
     ]
    }
   ],
   "source": [
    "import tensorflowjs as tfjs\n",
    "\n",
    "model.save('sarcasm.h5')\n",
    "tfjs.converters.save_keras_model(model, 'sarcasm.tfjs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
